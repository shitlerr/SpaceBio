# -*- coding: utf-8 -*-
"""data_preparation.py

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1cSTXkc8bBbxLCuQGoW-JoWlQi-J0lmpr
"""

import pandas as pd
import numpy as np
import re
import string
import json

df = pd.read_csv("space_biology_complete_dataset.csv")


def extract_sections_flexible(text):
    """
    More flexible section extraction that handles different text formats
    """
    if pd.isna(text) or not isinstance(text, str):
        return {
            'abstract': '',
            'methods': '',
            'results': '',
            'conclusion': ''
        }

    # Convert to lowercase for case-insensitive matching
    text_lower = text.lower()

    sections = {
        'abstract': '',
        'methods': '',
        'results': '',
        'conclusion': ''
    }

    # More flexible patterns with word boundaries and variations
    # Abstract - look for abstract section
    abstract_patterns = [
        r'\babstract\b\s*[:\-\n]*(.*?)(?=\b(introduction|background|methods|materials|results|discussion|conclusion|references)\b|$)',
        r'\bsummary\b\s*[:\-\n]*(.*?)(?=\b(introduction|background|methods|materials|results|discussion|conclusion|references)\b|$)'
    ]

    for pattern in abstract_patterns:
        match = re.search(pattern, text_lower, re.DOTALL | re.IGNORECASE)
        if match:
            sections['abstract'] = match.group(1).strip()
            break

    # Methods - look for methods section
    methods_patterns = [
        r'\b(methods|materials and methods|methodology|experimental procedures?)\b\s*[:\-\n]*(.*?)(?=\b(results|findings|discussion|conclusion|references)\b|$)',
        r'\bmaterials?\b\s*[:\-\n]*(.*?)(?=\b(results|findings|discussion|conclusion|references)\b|$)'
    ]

    for pattern in methods_patterns:
        match = re.search(pattern, text_lower, re.DOTALL | re.IGNORECASE)
        if match:
            # Use group 2 if pattern has two groups, otherwise group 1
            content = match.group(2) if len(match.groups()) > 1 else match.group(1)
            if content:  # ADD THIS CHECK
                sections['methods'] = content.strip()
            break

    # Results - look for results section
    results_patterns = [
        r'\b(results|findings|experimental results?)\b\s*[:\-\n]*(.*?)(?=\b(discussion|conclusion|references)\b|$)',
        r'\bfindings\b\s*[:\-\n]*(.*?)(?=\b(discussion|conclusion|references)\b|$)'
    ]

    for pattern in results_patterns:
        match = re.search(pattern, text_lower, re.DOTALL | re.IGNORECASE)
        if match:
            content = match.group(2) if len(match.groups()) > 1 else match.group(1)
            if content:  # ADD THIS CHECK
                sections['results'] = content.strip()
            break

    # Conclusion - look for conclusion/discussion
    conclusion_patterns = [
        r'\b(conclusion|discussion|concluding remarks)\b\s*[:\-\n]*(.*?)(?=\b(references|acknowledgments?|acknowledgements?)\b|$)',
        r'\bdiscussion\b\s*[:\-\n]*(.*?)(?=\b(references|acknowledgments?|acknowledgements?)\b|$)'
    ]

    for pattern in conclusion_patterns:
        match = re.search(pattern, text_lower, re.DOTALL | re.IGNORECASE)
        if match:
            content = match.group(2) if len(match.groups()) > 1 else match.group(1)
            if content:  # ADD THIS CHECK
                sections['conclusion'] = content.strip()
            break

    return sections

def debug_section_extraction(df, sample_size=5):
    """
    Debug function to see what's in the text and why extraction might be failing
    """
    print("DEBUGGING SECTION EXTRACTION:")
    print("=" * 50)

    for i in range(min(sample_size, len(df))):
        print(f"\n--- Document {i} ---")
        print(f"Title: {df.iloc[i]['title']}")
        text = df.iloc[i]['text']
        print(f"Text preview: {text[:500]}...")
        print(f"Text length: {len(text)}")

        # Check for common section headers
        text_lower = text.lower()
        sections_found = []
        for section in ['abstract', 'introduction', 'methods', 'materials', 'results', 'discussion', 'conclusion']:
            if section in text_lower:
                sections_found.append(section)

        print(f"Sections found: {sections_found}")

        # Test extraction
        extracted = extract_sections_flexible(text)
        print("Extracted sections:")
        for section, content in extracted.items():
            print(f"  {section}: {'‚úì' if content else '‚úó'} ({len(content)} chars)")

def create_chunked_dataframe_improved(df, text_column='text', title_column='title'):
    """
    Create chunked dataframe with improved extraction
    """
    # First, debug to see what we're working with
    debug_section_extraction(df)

    chunked_data = []

    for idx, row in df.iterrows():
        title = row[title_column] if title_column in df.columns and pd.notna(row[title_column]) else ""
        text_content = row[text_column] if text_column in df.columns and pd.notna(row[text_column]) else ""

        sections = extract_sections_flexible(text_content)

        chunked_row = {
            'original_index': idx,
            'title': title,
            'abstract': sections['abstract'],
            'methods': sections['methods'],
            'results': sections['results'],
            'conclusion': sections['conclusion'],
            'text_length': len(str(text_content)),
            'has_abstract': len(sections['abstract']) > 0,
            'has_methods': len(sections['methods']) > 0,
            'has_results': len(sections['results']) > 0,
            'has_conclusion': len(sections['conclusion']) > 0
        }
        chunked_data.append(chunked_row)

    return pd.DataFrame(chunked_data)

# Alternative: Simple keyword-based approach if regex fails
def extract_sections_simple(text):
    """
    Simple section extraction using keyword splitting
    """
    if pd.isna(text) or not isinstance(text, str):
        return {'abstract': '', 'methods': '', 'results': '', 'conclusion': ''}

    text_lower = text.lower()
    sections = {'abstract': '', 'methods': '', 'results': '', 'conclusion': ''}

    # Define section boundaries
    section_keywords = {
        'abstract': ['abstract', 'summary'],
        'methods': ['methods', 'materials and methods', 'methodology', 'materials'],
        'results': ['results', 'findings'],
        'conclusion': ['conclusion', 'discussion']
    }

    # Find positions of section headers
    positions = []
    for section, keywords in section_keywords.items():
        for keyword in keywords:
            pos = text_lower.find(keyword)
            if pos != -1:
                positions.append((pos, section, keyword))

    # Sort by position
    positions.sort()

    # Extract content between sections
    for i in range(len(positions)):
        current_pos, current_section, current_keyword = positions[i]

        # Find start of content (after the keyword)
        content_start = current_pos + len(current_keyword)

        # Find end of content (next section or end of text)
        if i < len(positions) - 1:
            next_pos = positions[i+1][0]
            content = text[content_start:next_pos].strip()
        else:
            content = text[content_start:].strip()

        # Clean up the content (remove any remaining headers)
        content = re.sub(r'^\W*\w+\W*', '', content).strip()

        if not sections[current_section]:
            sections[current_section] = content

    return sections

# Try the improved version
print("Attempting improved section extraction...")
df_chunked_improved = create_chunked_dataframe_improved(df, text_column='text', title_column='title')



# If that doesn't work, try the simple approach
print("\n" + "="*50)
print("Trying simple keyword-based approach...")

chunked_data_simple = []
for idx, row in df.iterrows():
    sections = extract_sections_simple(row['text'])
    chunked_row = {
        'original_index': idx,
        'title': row['title'],
        'abstract': sections['abstract'],
        'methods': sections['methods'],
        'results': sections['results'],
        'conclusion': sections['conclusion'],
        'has_abstract': len(sections['abstract']) > 0,
        'has_methods': len(sections['methods']) > 0,
        'has_results': len(sections['results']) > 0,
        'has_conclusion': len(sections['conclusion']) > 0
    }
    chunked_data_simple.append(chunked_row)

df_chunked_simple = pd.DataFrame(chunked_data_simple)

# Results
print(f"\nSimple approach results:")
print(f"Abstracts: {df_chunked_simple['has_abstract'].sum()}/{len(df_chunked_simple)}")
print(f"Methods: {df_chunked_simple['has_methods'].sum()}/{len(df_chunked_simple)}")
print(f"Results: {df_chunked_simple['has_results'].sum()}/{len(df_chunked_simple)}")
print(f"Conclusions: {df_chunked_simple['has_conclusion'].sum()}/{len(df_chunked_simple)}")

print("\nFirst few rows:")
print(df_chunked_simple[['title', 'has_abstract', 'has_methods', 'has_results', 'has_conclusion']].head(10))

# Save successful results
if df_chunked_simple['has_abstract'].sum() > 0:
    df_chunked_simple.to_csv('space_biology_chunked_success.csv', index=False)
    print(f"\nSaved {df_chunked_simple['has_abstract'].sum()} documents with extracted sections")
else:
    print("\nNo sections extracted. Let me examine the text structure more...")
    # Show sample of actual text content
    print("\nSample text content from first document:")
    print(df.iloc[0]['text'][:1000])

# 1. Convert your chunked data to training format
def prepare_training_data(df_chunked_simple):
    training_data = []

    for idx, row in df_chunked_simple.iterrows():
        # Create training examples from each document
        if row['abstract']:
            training_data.append({
                "role": "user",
                "content": f"Summarize this space biology research: {row['title']}"
            })
            training_data.append({
                "role": "assistant",
                "content": row['abstract']
            })

        if row['methods']:
            training_data.append({
                "role": "user",
                "content": f"What methods were used in: {row['title']}"
            })
            training_data.append({
                "role": "assistant",
                "content": row['methods']
            })

        if row['results']:
            training_data.append({
                "role": "user",
                "content": f"What were the key findings in: {row['title']}"
            })
            training_data.append({
                "role": "assistant",
                "content": row['results']
            })

    return training_data

# 2. Create the training data
print("Preparing training data...")
training_conversations = prepare_training_data(df_chunked_simple)
print(f"Created {len(training_conversations)} training messages")

# 3. Save the training data
with open('space_biology_training_data.json', 'w') as f:
    json.dump(training_conversations, f, indent=2)

print("Training data saved to 'space_biology_training_data.json'")

# OLD VERSION - COMMENTED OUT (too large for API)
# def save_knowledge_base(df_chunked_simple):
#     knowledge_lines = []

#     for idx, row in df_chunked_simple.iterrows():
#         if row['abstract']:
#             knowledge_lines.append(f"TITLE: {row['title']}")
#             knowledge_lines.append(f"ABSTRACT: {row['abstract']}")
#             knowledge_lines.append("---")

#         if row['methods']:
#             knowledge_lines.append(f"TITLE: {row['title']}")
#             knowledge_lines.append(f"METHODS: {row['methods']}")
#             knowledge_lines.append("---")

#         if row['results']:
#             knowledge_lines.append(f"TITLE: {row['title']}")
#             knowledge_lines.append(f"RESULTS: {row['results']}")
#             knowledge_lines.append("---")

#     # Save to file
#     with open("knowledge.txt", "w", encoding="utf-8") as f:
#         f.write("\n".join(knowledge_lines))

#     print(f"Saved knowledge base with {len(knowledge_lines)//3} documents")

# Run this to create your knowledge base
# save_knowledge_base(df_chunked_simple)  # COMMENTED OUT - TOO LARGE

# OPTIMIZED VERSION - USE THIS ONE
def save_optimized_knowledge_base(df_chunked_simple, max_chars=200000):
    """
    Create a smaller, optimized knowledge base that fits within token limits
    """
    knowledge_lines = []
    total_chars = 0
    
    # Sort by most complete documents (those with abstract, methods, and results)
    df_chunked_simple['completeness_score'] = (
        df_chunked_simple['has_abstract'].astype(int) +
        df_chunked_simple['has_methods'].astype(int) +
        df_chunked_simple['has_results'].astype(int)
    )
    
    # Get the most complete documents first
    sorted_df = df_chunked_simple.sort_values('completeness_score', ascending=False)
    
    for idx, row in sorted_df.iterrows():
        doc_lines = []
        
        # Add title
        doc_lines.append(f"TITLE: {row['title']}")
        
        # Add abstract (truncated if too long)
        if row['abstract']:
            abstract = row['abstract'][:1000] + "..." if len(row['abstract']) > 1000 else row['abstract']
            doc_lines.append(f"ABSTRACT: {abstract}")
        
        # Add methods (truncated)
        if row['methods']:
            methods = row['methods'][:800] + "..." if len(row['methods']) > 800 else row['methods']
            doc_lines.append(f"METHODS: {methods}")
        
        # Add results (truncated)
        if row['results']:
            results = row['results'][:800] + "..." if len(row['results']) > 800 else row['results']
            doc_lines.append(f"RESULTS: {results}")
        
        doc_lines.append("---")
        
        # Check if adding this document would exceed our limit
        doc_text = "\n".join(doc_lines)
        if total_chars + len(doc_text) > max_chars:
            break
            
        knowledge_lines.extend(doc_lines)
        total_chars += len(doc_text)
    
    # Save to file
    with open("space_biology_knowledge_optimized.txt", "w", encoding="utf-8") as f:
        f.write("\n".join(knowledge_lines))
    
    print(f"‚úÖ Saved optimized knowledge base with {len(knowledge_lines)//4} documents")
    print(f"üìä Total size: {total_chars} characters")
    print(f"üìù Documents included: {len([x for x in knowledge_lines if x.startswith('TITLE:')])}")

# Run this instead of save_knowledge_base
print("\nCreating optimized knowledge base for chatbot...")
save_optimized_knowledge_base(df_chunked_simple)
print("‚úÖ Data preparation complete! Use 'space_biology_knowledge_optimized.txt' for your chatbot.")
